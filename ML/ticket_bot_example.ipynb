{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3389b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "import xlrd\n",
    "import psycopg2\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dcdc7fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/darya.mishina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import pymorphy2\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from autocorrect import Speller\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa27f71",
   "metadata": {},
   "source": [
    "## загружаем данные для обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35326aa",
   "metadata": {},
   "source": [
    "### вариант загрузки из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db670f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603b0c4",
   "metadata": {},
   "source": [
    "### вариант загрузки из БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#убрала параметры подключения из-за конфиденциальности\n",
    "config = {\"conn_string\": {\"host\": \"\", \"port\": \"\", \"dbname\": \"\",\n",
    "                         \"user\": \"\", \"password\": \"\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aea988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql(sql, error=0, conf = config):\n",
    "    conn = psycopg2.connect(**conf.get('conn_string'))\n",
    "    try:\n",
    "        df = pd.read_sql(sql,conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    except Exception as t:\n",
    "        print(t)\n",
    "        if error<100:\n",
    "            error+=1\n",
    "            conn.close()\n",
    "            time.sleep(1.5)\n",
    "            return read_sql(sql = sql,error = error,conf = conf)\n",
    "        else:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#убрала скрипт из-за конфиденциальности\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    \n",
    "FROM \n",
    "\n",
    "WHERE \n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e0209",
   "metadata": {},
   "source": [
    "## классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2367abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_define(data):\n",
    "    target_data = data[data['Категория обращения 2'].isna()!=True][['Подробное описание','Категория обращения 2']]\n",
    "    enc = LabelEncoder()\n",
    "    target_data['teg'] = enc.fit_transform(target_data['Категория обращения 2'])\n",
    "    a = target_data.groupby('teg').teg.count().sort_values()\n",
    "    classes = list(a[a>20].index)\n",
    "    target_data['teg'] = target_data['teg'].apply(lambda x: x if x in classes else 99)\n",
    "    target_data_1 = target_data\n",
    "    return target_data, target_data_1, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f1afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data, target_data_1, classes = classes_define(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56b714",
   "metadata": {},
   "source": [
    "## подготовка для TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0091211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and len(token)>=3 \\\n",
    "              and token.strip() not in punctuation \\\n",
    "              and token.isdigit()==False]\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0ec054",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.extend(['лента','ассорт','разм','арт','что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'на'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41765ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_preproc(target_data_1):\n",
    "    target_data_1['Подробное описание_11'] = target_data_1['Подробное описание'].apply(lambda x: re.sub(r'[^a-zA-Zа-яА-Я 0-9]+','',x))\n",
    "    target_data_1['processed_lem']=target_data_1['Подробное описание_11'].apply(preprocess_text)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(target_data_1['processed_lem'],target_data_1['teg'], test_size = 0.5)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ad2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = tfidf_preproc(target_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b79c67",
   "metadata": {},
   "source": [
    "## модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d05ae0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model(x_train, y_train):\n",
    "    \n",
    "    tf_idf = TfidfVectorizer(stop_words=russian_stopwords, max_features=50000, min_df=2)\n",
    "    \n",
    "    logit = LogisticRegression(C=1e2, n_jobs=4, solver='lbfgs', \n",
    "                           random_state=17, verbose=0, \n",
    "                           multi_class='multinomial',\n",
    "                           fit_intercept=True, max_iter=400)\n",
    "   \n",
    "    tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n",
    "                                 ('logit', logit)])\n",
    "    tfidf_logit_pipeline.fit(x_train, y_train)\n",
    "    return tfidf_logit_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ecef074",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_logit_pipeline = tfidf_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15793a",
   "metadata": {},
   "source": [
    "## файл, который в бот отправляют пользователи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b86a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_excel('Декабрь.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27253b",
   "metadata": {},
   "source": [
    "## обрабатываем файл из бота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "817e8c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preproc(valid):\n",
    "    valid['Подробное описание'] = valid['Подробное описание'].apply(str)\n",
    "    valid['Подробное описание_11'] = valid['Подробное описание'].apply(lambda x: re.sub(r'[^a-zA-Zа-яА-Я 0-9]+','',x))\n",
    "    valid['processed_lem']=valid['Подробное описание_11'].apply(preprocess_text)\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9737e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = test_preproc(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6932ba",
   "metadata": {},
   "source": [
    "## Предсказываем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f910cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(valid, tfidf_logit_pipeline):\n",
    "    x_valid = valid['processed_lem']\n",
    "    valid_pred_final = tfidf_logit_pipeline.predict(x_valid)\n",
    "    valid['teg'] = valid_pred_final\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "508c95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = predict_classes(valid, tfidf_logit_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774b9bf",
   "metadata": {},
   "source": [
    "## подтягиваем к тегу категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67a569d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_add(target_data, data, classes):\n",
    "    name1_teg = target_data[['Категория обращения 2', 'teg']]\n",
    "    name1_teg = name1_teg.drop_duplicates()\n",
    "    \n",
    "    name2_teg =data[['Категория обращения 1', 'Категория обращения 2']]\n",
    "    name2_teg = name2_teg.drop_duplicates()\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Проблемы в работе оборудования'] = \"3. Оборудование\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Проблемы с товарами'] = \"2. BO\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Настройка рабочего места'] = \"3. Оборудование\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Сетевые папки'] = \"4. Программное обеспечение\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Редактирование заказов'] = \"2. BO\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Уточнение статуса или решения'] = \"6. Запрос решения по обращению\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Проблемы с бонусными баллами и картами лояльности'] = \"7. www.vprok.ru\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Проблемы в работе приложения'] = \"8. Мобильное приложение\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Учётные записи'] = \"3. Оборудование\"\n",
    "    name2_teg['Категория обращения 1'][name2_teg['Категория обращения 2'] == 'Проблемы с адресами доставки'] = \"7. www.vprok.ru\"\n",
    "    name2_teg = name2_teg.drop_duplicates()\n",
    "    name_teg = pd.merge(name1_teg, \n",
    "                      name2_teg, \n",
    "                      on ='Категория обращения 2', \n",
    "                      how ='inner')\n",
    "\n",
    "    name_teg['teg'] = name_teg['teg'].apply(lambda x: x if x in classes else 99)\n",
    "    #меняем название тэгов 99 на прочее\n",
    "    name_teg['Категория обращения 2'][name_teg.teg == 99] = \"Прочее\"\n",
    "    name_teg['Категория обращения 1'][name_teg.teg == 99] = \"Прочее\"\n",
    "    name_teg = name_teg.drop_duplicates()\n",
    "    return name_teg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "810eda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_teg = cat_add(target_data, df, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2edeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(valid, \n",
    "                    name_teg, \n",
    "                   on ='teg', \n",
    "                   how ='inner')\n",
    "final.drop(columns=['Подробное описание_11', 'processed_lem'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a222706",
   "metadata": {},
   "source": [
    "## или выгружаем в эксель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21b4f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = ''\n",
    "filename = 'tickets_support_classification_Декабрь_3.xlsx' \n",
    "final.to_excel(os.path.join(path_to_save, filename), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad6efa",
   "metadata": {},
   "source": [
    "## или пишем в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8949b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция ктр записывает данные\n",
    "def metrics(df, table, varible = 'date', drop = False):\n",
    "    if df.empty == False:\n",
    "        df[varible] = pd.to_datetime(df[varible])\n",
    "        d1 = df[varible].min().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        d2 = (df[varible].max()).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        condition = datetime.now()\n",
    "        df['version'] = condition\n",
    "        conn = psycopg2.connect(**config.get(\"conn_mybase\"))\n",
    "        df_columns = list(df)\n",
    "        columns = \",\".join(df_columns)\n",
    "        values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns]))\n",
    "        cur = conn.cursor()\n",
    "        insert_stmt = \"INSERT INTO {} ({}) {}\".format(table, columns, values)\n",
    "        if drop!=False:\n",
    "            drop_table = \"DELETE from {}\".format(table)\n",
    "            cur.execute(drop_table)\n",
    "        execute_batch(cur, insert_stmt, df.values)\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#вот здесь указываем схему и таблицу будет \n",
    "metrics(final, 'scheme.table', varible = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8072a9",
   "metadata": {},
   "source": [
    "## или отправляем все в бот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dbaf095",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN  = '5063.......' #убрала часть токена\n",
    "\n",
    "bot = telebot.TeleBot(TOKEN)\n",
    "\n",
    "\n",
    "### Прием документов\n",
    "@bot.message_handler(content_types=['document'])\n",
    "\n",
    "def handle_docs(message):\n",
    "    if message.chat.username in ['Daria_Mishina28']: #здесь указать публичные имена пользователей в тг\n",
    "        print('try1')\n",
    "        try:\n",
    "            file_info = bot.get_file(message.document.file_id)\n",
    "            downloaded_file = bot.download_file(file_info.file_path)\n",
    "            print('downloaded_file compplete')\n",
    "            #отдельная папка для присланных файлов\n",
    "            src = 'data/' +message.chat.username +'_'+ str(datetime.now()) +'_'+ message.document.file_name \n",
    "            with open(src, 'wb') as new_file:\n",
    "                new_file.write(downloaded_file)\n",
    "            try:\n",
    "                print(src)\n",
    "                data = pd.read_excel(src)\n",
    "                print('data загружена')\n",
    "                #файл для обучения\n",
    "                df = pd.read_csv('Статистика по 1 линии (Q2) - размеченные данные.csv')\n",
    "                print('df загружен')\n",
    "                #классы\n",
    "                target_data, target_data_1, classes = classes_define(df)\n",
    "                print('классы готовы')\n",
    "                #подготовка для TF-IDF\n",
    "                x_train, x_test, y_train, y_test = tfidf_preproc(target_data_1)\n",
    "                print('x_train, x_test готов')\n",
    "                #модель\n",
    "                tfidf_logit_pipeline = tfidf_model(x_train, y_train)\n",
    "                print('модель готова')\n",
    "                #обрабатываем файл из тг\n",
    "                valid = test_preproc(data)\n",
    "                print('файл из тг обработан')\n",
    "                #предсказываем\n",
    "                valid = predict_classes(valid, tfidf_logit_pipeline)\n",
    "                print('предсказано')\n",
    "                #обрабатываем предсказания для выгрузки\n",
    "                name_teg = cat_add(target_data, df, classes)\n",
    "                print('категории сформированы')\n",
    "                #подтягиваем категории к предсказаниям\n",
    "                final = pd.merge(valid, \n",
    "                      name_teg, \n",
    "                      on ='teg', \n",
    "                      how ='inner')\n",
    "                final.drop(columns=['Подробное описание_11', 'processed_lem'], inplace=True)\n",
    "                print('категории соединены с предсказанием')\n",
    "                src2 = 'data/' + 'new_' + message.chat.username +'_'+ str(datetime.now()) + '_' + message.document.file_name\n",
    "                final.to_excel(src2, index=False)\n",
    "                print('предсказания готовы')\n",
    "                f = open(src2, 'rb')\n",
    "                bot.send_document(chat_id=message.chat.id, data=f, disable_notification=True) \n",
    "                print('файл отправлен в бот')\n",
    "                f.close()\n",
    "                #os.remove(src) #загруженные и обработанные файлы не удаляем\n",
    "                #os.remove(src2)\n",
    "            except:\n",
    "                bot.reply_to(message, \"что-то пошло не так\")\n",
    "        except Exception as e:\n",
    "            bot.reply_to(message, e)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "503c12f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try1\n",
      "downloaded_file compplete\n",
      "data/Daria_Mishina28_2021-12-24 16:38:29.063250_Прошлая неделя.xlsx\n",
      "data загружена\n",
      "df загружен\n",
      "классы готовы\n",
      "x_train, x_test готов\n",
      "модель готова\n",
      "файл из тг обработан\n",
      "предсказано\n",
      "категории сформированы\n",
      "категории соединены с предсказанием\n",
      "предсказания готовы\n",
      "файл отправлен в бот\n"
     ]
    }
   ],
   "source": [
    "bot.polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82123084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
