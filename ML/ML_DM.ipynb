{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification/ML', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages', '/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1006)>\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from tqdm import trange\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "\n",
    "my_stopwords = [] #заглушка если придумаем стопслова\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "russian_stopwords_new = stopwords.words(\"russian\")\n",
    "russian_stopwords_new.extend(my_stopwords)\n",
    "not_stopwords = {'не', 'ни'}\n",
    "russian_stopwords = [word for word in russian_stopwords_new if word not in not_stopwords]\n",
    "\n",
    "from sklearn.model_selection import  StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "from matplotlib import pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# добавляем в path папку с тем файлом, откуда надо импортировать функции\n",
    "sys.path.append('/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data_preproc.src.utils import preprocess_text1, preprocess_text2, preprocess_text3, preprocess_text4, tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"mlds23-authorship-identification\"\n",
    "BUCKET_DIR = \"splitted_data/\"\n",
    "FILENAME = 'splitted_df.csv'\n",
    "# FILENAME = 'splitted_w_ds_df.csv'\n",
    "seed = 42\n",
    "model_name = 'tfidf_logreg_pipeline_'+'preproc4_'+str(seed)\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification/ML/wandb/run-20231206_134415-9y8y34ll</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlds23_ai/authorship_identification/runs/9y8y34ll' target=\"_blank\">tfidf_logreg_pipeline_preproc4_42</a></strong> to <a href='https://wandb.ai/mlds23_ai/authorship_identification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlds23_ai/authorship_identification' target=\"_blank\">https://wandb.ai/mlds23_ai/authorship_identification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlds23_ai/authorship_identification/runs/9y8y34ll' target=\"_blank\">https://wandb.ai/mlds23_ai/authorship_identification/runs/9y8y34ll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mlds23_ai/authorship_identification/runs/9y8y34ll?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb810107ca0>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"authorship_identification\", name=model_name, entity=\"mlds23_ai\", tags=['baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config['random_state'] = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем уже расспличенные тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    endpoint_url='https://storage.yandexcloud.net',\n",
    "    aws_access_key_id='YCAJErlaldUmioGbHQSqJ70MR',\n",
    "    aws_secret_access_key='YCPSba_JgloNYSNWcnKO2CYCEB8PFR1Iwgr2jIUy',\n",
    "    region_name='ru-cental1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_obj = s3.get_object(Bucket=BUCKET_NAME, Key=BUCKET_DIR + FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(StringIO(csv_obj['Body'].read().decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружает в локальную директорию, потом отдельно надо считывать\n",
    "# s3.download_file(Filename=FILENAME, Bucket=BUCKET_NAME, Key=BUCKET_DIR + FILENAME)\n",
    "# df = pd.read_csv('splitted_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>Узкими горными тропинками , от одного дачного ...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>свернутый ковер для акробатических упражнений ...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>позабытые . Кроме того , были в шарманке две п...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>тайной грусти : — Что поделаешь ? .. Древний о...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>, да уж ладно ! Кормила она нас с тобой , Серг...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text  \\\n",
       "0  author_id_06  Узкими горными тропинками , от одного дачного ...   \n",
       "1  author_id_06  свернутый ковер для акробатических упражнений ...   \n",
       "2  author_id_06  позабытые . Кроме того , были в шарманке две п...   \n",
       "3  author_id_06  тайной грусти : — Что поделаешь ? .. Древний о...   \n",
       "4  author_id_06  , да уж ладно ! Кормила она нас с тобой , Серг...   \n",
       "\n",
       "                                         book  \n",
       "0  raw_data/aleksandr_kuprin_belyj_pudel'.txt  \n",
       "1  raw_data/aleksandr_kuprin_belyj_pudel'.txt  \n",
       "2  raw_data/aleksandr_kuprin_belyj_pudel'.txt  \n",
       "3  raw_data/aleksandr_kuprin_belyj_pudel'.txt  \n",
       "4  raw_data/aleksandr_kuprin_belyj_pudel'.txt  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Узкими',\n",
       " 'горными',\n",
       " 'тропинками',\n",
       " ',',\n",
       " 'от',\n",
       " 'одного',\n",
       " 'дачного',\n",
       " 'поселка',\n",
       " 'до',\n",
       " 'другого',\n",
       " ',',\n",
       " 'пробиралась',\n",
       " 'вдоль',\n",
       " 'южного',\n",
       " 'берега',\n",
       " 'Крыма',\n",
       " 'маленькая',\n",
       " 'бродячая',\n",
       " 'труппа',\n",
       " '.',\n",
       " 'Впереди',\n",
       " 'обыкновенно',\n",
       " 'бежал',\n",
       " ',',\n",
       " 'свесив',\n",
       " 'набок',\n",
       " 'длинный',\n",
       " 'розовый',\n",
       " 'язык',\n",
       " ',',\n",
       " 'белый',\n",
       " 'пудель',\n",
       " 'Арто',\n",
       " ',',\n",
       " 'остриженный',\n",
       " 'наподобие',\n",
       " 'льва',\n",
       " '.',\n",
       " 'У',\n",
       " 'перекрестков',\n",
       " 'он',\n",
       " 'останавливался',\n",
       " 'и',\n",
       " ',',\n",
       " 'махая',\n",
       " 'хвостом',\n",
       " ',',\n",
       " 'вопросительно',\n",
       " 'оглядывался',\n",
       " 'назад',\n",
       " '.',\n",
       " 'По',\n",
       " 'каким-то',\n",
       " 'ему',\n",
       " 'одному',\n",
       " 'известным',\n",
       " 'признакам',\n",
       " 'он',\n",
       " 'всегда',\n",
       " 'безошибочно',\n",
       " 'узнавал',\n",
       " 'дорогу',\n",
       " 'и',\n",
       " ',',\n",
       " 'весело',\n",
       " 'болтая',\n",
       " 'мохнатыми',\n",
       " 'ушами',\n",
       " ',',\n",
       " 'кидался',\n",
       " 'галопом',\n",
       " 'вперед',\n",
       " '.',\n",
       " 'За',\n",
       " 'собакой',\n",
       " 'шел',\n",
       " 'двенадцатилетний',\n",
       " 'мальчик',\n",
       " 'Сергей',\n",
       " ',']"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Выделяем классы метками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "df['author_id'] = enc.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_id\n",
       "1     379\n",
       "7     446\n",
       "8     627\n",
       "5     725\n",
       "3     804\n",
       "6    1047\n",
       "4    1295\n",
       "0    1299\n",
       "9    2387\n",
       "2    2706\n",
       "Name: author_id, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#посмотрим на распределение классов\n",
    "df.groupby('author_id').author_id.count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# подготовка для TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#удаляем цифры, http, приводим к нижнему регистру, убираем пробелы, короткие слова и пунктуацию+лемматизация\n",
    "df['text_proc1'] = df['text'].apply(lambda x: preprocess_text1(x, tostr=True))\n",
    "#удаляем цифры, http, приводим к нижнему регистру, убираем пробелы, короткие слова. ОСТАВЛЯЕМ пунктуацию+лемматизация\n",
    "df['text_proc2'] = df['text'].apply(lambda x: preprocess_text2(x, tostr=True))\n",
    "#удаляем цифры, http, приводим к нижнему регистру, убираем пробелы, пунктуацию ОСТАВЛЯЕМ короткие слова.+лемматизация\n",
    "df['text_proc3'] = df['text'].apply(lambda x: preprocess_text3(x, tostr=True))\n",
    "#удаляем цифры, http, приводим к нижнему регистру, убираем пробелы, пунктуацию ОСТАВЛЯЕМ короткие слова и стопслова +лемматизация\n",
    "df['text_proc4'] = df['text'].apply(lambda x: preprocess_text4(x, tostr=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>book</th>\n",
       "      <th>book_share</th>\n",
       "      <th>split</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text_proc1</th>\n",
       "      <th>text_proc2</th>\n",
       "      <th>text_proc3</th>\n",
       "      <th>text_proc4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>Узкими горными тропинками , от одного дачного ...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>0.109838</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>узкий горный тропинка дачный поселок пробирать...</td>\n",
       "      <td>узкий горный тропинка  ,   дачный поселок  ,  ...</td>\n",
       "      <td>узкий горный тропинка дачный поселок пробирать...</td>\n",
       "      <td>узкий горный тропинка от один дачный поселок д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>свернутый ковер для акробатических упражнений ...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>0.109838</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>свернутый ковер акробатический упражнение прав...</td>\n",
       "      <td>свернутый ковер акробатический упражнение  ,  ...</td>\n",
       "      <td>свернутый ковер акробатический упражнение прав...</td>\n",
       "      <td>свернутый ковер для акробатический упражнение ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>позабытые . Кроме того , были в шарманке две п...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>0.109838</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>позабывать кроме шарманка предательский труба ...</td>\n",
       "      <td>позабывать кроме  ,  шарманка предательский тр...</td>\n",
       "      <td>позабывать кроме шарманка предательский труба ...</td>\n",
       "      <td>позабывать кроме то быть в шарманка два предат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>тайной грусти : — Что поделаешь ? .. Древний о...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>0.109838</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>тайный грусть   —  поделать древний орган прос...</td>\n",
       "      <td>тайный грусть  : —  поделать древний орган ......</td>\n",
       "      <td>тайный грусть   —  поделать древний орган прос...</td>\n",
       "      <td>тайный грусть   —  что поделать древний орган ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>, да уж ладно ! Кормила она нас с тобой , Серг...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>0.109838</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>ладно кормить сергей сей пора бог давать покор...</td>\n",
       "      <td>,    ладно кормить  ,  сергей  ,   сей пора  ,...</td>\n",
       "      <td>ладно кормить сергей сей пора бог давать покор...</td>\n",
       "      <td>да уж ладно кормить она мы с ты сергей до сей ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text  \\\n",
       "0  author_id_06  Узкими горными тропинками , от одного дачного ...   \n",
       "1  author_id_06  свернутый ковер для акробатических упражнений ...   \n",
       "2  author_id_06  позабытые . Кроме того , были в шарманке две п...   \n",
       "3  author_id_06  тайной грусти : — Что поделаешь ? .. Древний о...   \n",
       "4  author_id_06  , да уж ладно ! Кормила она нас с тобой , Серг...   \n",
       "\n",
       "                                         book  book_share split  author_id  \\\n",
       "0  raw_data/aleksandr_kuprin_belyj_pudel'.txt    0.109838  test          6   \n",
       "1  raw_data/aleksandr_kuprin_belyj_pudel'.txt    0.109838  test          6   \n",
       "2  raw_data/aleksandr_kuprin_belyj_pudel'.txt    0.109838  test          6   \n",
       "3  raw_data/aleksandr_kuprin_belyj_pudel'.txt    0.109838  test          6   \n",
       "4  raw_data/aleksandr_kuprin_belyj_pudel'.txt    0.109838  test          6   \n",
       "\n",
       "                                          text_proc1  \\\n",
       "0  узкий горный тропинка дачный поселок пробирать...   \n",
       "1  свернутый ковер акробатический упражнение прав...   \n",
       "2  позабывать кроме шарманка предательский труба ...   \n",
       "3  тайный грусть   —  поделать древний орган прос...   \n",
       "4  ладно кормить сергей сей пора бог давать покор...   \n",
       "\n",
       "                                          text_proc2  \\\n",
       "0  узкий горный тропинка  ,   дачный поселок  ,  ...   \n",
       "1  свернутый ковер акробатический упражнение  ,  ...   \n",
       "2  позабывать кроме  ,  шарманка предательский тр...   \n",
       "3  тайный грусть  : —  поделать древний орган ......   \n",
       "4  ,    ладно кормить  ,  сергей  ,   сей пора  ,...   \n",
       "\n",
       "                                          text_proc3  \\\n",
       "0  узкий горный тропинка дачный поселок пробирать...   \n",
       "1  свернутый ковер акробатический упражнение прав...   \n",
       "2  позабывать кроме шарманка предательский труба ...   \n",
       "3  тайный грусть   —  поделать древний орган прос...   \n",
       "4  ладно кормить сергей сей пора бог давать покор...   \n",
       "\n",
       "                                          text_proc4  \n",
       "0  узкий горный тропинка от один дачный поселок д...  \n",
       "1  свернутый ковер для акробатический упражнение ...  \n",
       "2  позабывать кроме то быть в шарманка два предат...  \n",
       "3  тайный грусть   —  что поделать древний орган ...  \n",
       "4  да уж ладно кормить она мы с ты сергей до сей ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model_with_cv(df):\n",
    "    # russian_stopwords = russian_stopwords \n",
    "    max_features = 550\n",
    "    min_df = 2\n",
    "    # seed = seed \n",
    "    # test_size = test_size\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df['text_proc4'], df['author_id'], test_size = test_size, random_state=seed,\\\n",
    "                                                    stratify=df['author_id'])\n",
    "\n",
    "    tf_idf = TfidfVectorizer(stop_words=russian_stopwords, \n",
    "                             # max_features=max_features, min_df=min_df\n",
    "                            )\n",
    "\n",
    "\n",
    "    classifier = LogisticRegression(C=1e2, n_jobs=4, solver='sag',\n",
    "                               random_state=seed, verbose=0,\n",
    "                               multi_class='multinomial',\n",
    "                               fit_intercept=True, max_iter=400)\n",
    "\n",
    "\n",
    "    model = Pipeline([('tf_idf', tf_idf),\n",
    "                    ('classifier', classifier)])\n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    \n",
    "\n",
    "    wandb.config['dataset'] = FILENAME\n",
    "\n",
    "    wandb.config['preprocessing'] = {\n",
    "        'lowercase': True,\n",
    "        'remove_punct': True,\n",
    "        'remove_html_tags': True,\n",
    "        'remove_numbers': True,\n",
    "        'lemmatization': True,\n",
    "        'remove_short_words': False,\n",
    "        'remove_stop_words': False,\n",
    "    }\n",
    "    wandb.config['embedding'] = {\n",
    "        'embedding_type': 'TF-IDF'    \n",
    "    }\n",
    "\n",
    "    wandb.config['classifier'] = {\n",
    "        'classifier_name': model['classifier'].__class__.__name__,\n",
    "        'solver': model['classifier'].solver,\n",
    "        'max_iter': model['classifier'].max_iter,\n",
    "        'C': model['classifier'].C,\n",
    "        'n_jobs': model['classifier'].n_jobs,\n",
    "        'multi_class': model['classifier'].multi_class,\n",
    "        'fit_intercept': model['classifier'].fit_intercept,\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "    wandb.config['evaluation'] = {\n",
    "        'test_size': test_size,\n",
    "        'train_cv': skf.n_splits\n",
    "    }\n",
    "\n",
    "\n",
    "    cv_results = cross_validate(model, x_train, y_train,\n",
    "                                scoring=['f1_micro', 'f1_macro', 'f1_weighted'],\n",
    "                                cv=skf)\n",
    "\n",
    "\n",
    "    print(\"F1 Micro:\", cv_results['test_f1_micro'].mean())\n",
    "    print(\"F1 Macro:\", cv_results['test_f1_macro'].mean())\n",
    "    print(\"F1 Weighted:\", cv_results['test_f1_weighted'].mean())\n",
    "    \n",
    "\n",
    "    metrics_train = dict()\n",
    "\n",
    "    for metric in ['test_f1_micro', 'test_f1_macro', 'test_f1_weighted']:\n",
    "        metrics_train[f'{metric[5:]}_mean'] = cv_results[metric].mean()\n",
    "        metrics_train[f'{metric[5:]}_std'] = cv_results[metric].std()\n",
    "        \n",
    "    wandb.log({\n",
    "    'train': metrics_train\n",
    "    })\n",
    "        \n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    return model, metrics_train, y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.8934146341463416\n",
      "F1 Macro: 0.8900527979995052\n",
      "F1 Weighted: 0.8926013385041\n"
     ]
    }
   ],
   "source": [
    "model, metrics_train, y_pred   = tfidf_model_with_cv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tf_idf',\n",
       "                 TfidfVectorizer(stop_words=['и', 'в', 'во', 'что', 'он', 'на',\n",
       "                                             'я', 'с', 'со', 'как', 'а', 'то',\n",
       "                                             'все', 'она', 'так', 'его', 'но',\n",
       "                                             'да', 'ты', 'к', 'у', 'же', 'вы',\n",
       "                                             'за', 'бы', 'по', 'только', 'ее',\n",
       "                                             'мне', 'было', ...])),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=100.0, max_iter=400,\n",
       "                                    multi_class='multinomial', n_jobs=4,\n",
       "                                    random_state=42, solver='sag'))])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_micro_mean': 0.8934146341463416,\n",
       " 'f1_micro_std': 0.004838252298750583,\n",
       " 'f1_macro_mean': 0.8900527979995052,\n",
       " 'f1_macro_std': 0.006902788931788446,\n",
       " 'f1_weighted_mean': 0.8926013385041,\n",
       " 'f1_weighted_std': 0.005130526215383625}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_test, y_pred, df):\n",
    "\n",
    "    metrics_test = dict()\n",
    "\n",
    "    for average in ['micro', 'weighted', 'macro']:\n",
    "        metrics_test[f'f1_{average}'] = f1_score(y_test, y_pred, average=average)\n",
    "        \n",
    "    wandb.log({\n",
    "    'test': metrics_test})\n",
    "\n",
    "    precision_recall_f1_test_df = pd.DataFrame(\n",
    "        precision_recall_fscore_support(y_test, y_pred),\n",
    "        columns=df.author_id.unique().tolist(),\n",
    "        index = ['precision_test', 'recall_test', 'f1_test', 'support']).T.drop(columns=['support']).reset_index()\n",
    "\n",
    "    precision_recall_f1_test_df.rename(columns={'index': 'author'}, inplace=True)\n",
    "    wandb.log({\n",
    "    'precision_recall_f1_table_test': wandb.Table(dataframe=precision_recall_f1_test_df)})\n",
    "    \n",
    "    wandb.log({\n",
    "    'conf_mat_test': wandb.plot.confusion_matrix(probs=None, y_true=y_test.tolist(), preds=y_pred, class_names=df.author_id.unique().tolist())   \n",
    "})\n",
    "    \n",
    "    return metrics_test, precision_recall_f1_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test, precision_recall_f1_test_df = calc_metrics(y_test, y_pred, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.9069701280227596,\n",
       " 'f1_weighted': 0.9060874665840136,\n",
       " 'f1_macro': 0.9032614957378018}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.968586</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.958549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.991071</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.982301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.886705</td>\n",
       "      <td>0.944581</td>\n",
       "      <td>0.914729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>0.856512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.933162</td>\n",
       "      <td>0.921320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.937853</td>\n",
       "      <td>0.764977</td>\n",
       "      <td>0.842640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.888179</td>\n",
       "      <td>0.885350</td>\n",
       "      <td>0.886762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.938462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.899371</td>\n",
       "      <td>0.760638</td>\n",
       "      <td>0.824207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0.875325</td>\n",
       "      <td>0.941341</td>\n",
       "      <td>0.907133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author  precision_test  recall_test   f1_test\n",
       "0       6        0.968586     0.948718  0.958549\n",
       "1       0        0.991071     0.973684  0.982301\n",
       "2       7        0.886705     0.944581  0.914729\n",
       "3       3        0.915094     0.804979  0.856512\n",
       "4       1        0.909774     0.933162  0.921320\n",
       "5       9        0.937853     0.764977  0.842640\n",
       "6       5        0.888179     0.885350  0.886762\n",
       "7       2        0.968254     0.910448  0.938462\n",
       "8       4        0.899371     0.760638  0.824207\n",
       "9       8        0.875325     0.941341  0.907133"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f1_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tfidf_logreg_pipeline_preproc4_42</strong> at: <a href='https://wandb.ai/mlds23_ai/authorship_identification/runs/9y8y34ll' target=\"_blank\">https://wandb.ai/mlds23_ai/authorship_identification/runs/9y8y34ll</a><br/> View job at <a href='https://wandb.ai/mlds23_ai/authorship_identification/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExODg5MjQyOQ==/version_details/v4' target=\"_blank\">https://wandb.ai/mlds23_ai/authorship_identification/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExODg5MjQyOQ==/version_details/v4</a><br/>Synced 6 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231206_134415-9y8y34ll/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## просто пайплайн без логирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем добавить регуляризацию к логистической регрессии\n",
    "# classifier = LogisticRegression(C=0.5, n_jobs=4, solver='sag', random_state=seed, \n",
    "verbose=0, multi_class='multinomial', fit_intercept=True, max_iter=400, penalty='l2')\n",
    "\n",
    "# Попробуем уменьшить количество признаков для TF-IDF\n",
    "# max_features = 300\n",
    "\n",
    "# Попробуем уменьшить степень регуляризации\n",
    "# C = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model_with_cv(df, split = True):\n",
    "    # russian_stopwords = russian_stopwords \n",
    "    max_features = 500 \n",
    "    min_df = 2\n",
    "    # seed = seed \n",
    "    # test_size= test_size\n",
    "\n",
    "    if not split: \n",
    "        x_train, x_test, y_train, y_test = train_test_split(df['text_proc4'], df['author_id'], test_size = test_size, random_state=seed,\\\n",
    "                                                    stratify=df['author_id'])\n",
    "\n",
    "    else:\n",
    "        x_test = df[df.split == 'test']['text_proc1']\n",
    "        x_train = df[df.split == 'train']['text_proc1']\n",
    "        y_test = df[df.split == 'test']['author_id']\n",
    "        y_train = df[df.split == 'train']['author_id']\n",
    "\n",
    "    tf_idf = TfidfVectorizer(stop_words=russian_stopwords, \n",
    "                               max_features=max_features, \n",
    "                             min_df=min_df\n",
    "                            )\n",
    "\n",
    "\n",
    "    classifier = LogisticRegression(C=1e2, n_jobs=4, solver='sag',\n",
    "                               random_state=seed, verbose=0,\n",
    "                               multi_class='multinomial',\n",
    "                               fit_intercept=True, max_iter=400,\n",
    "                                   penalty='l2'\n",
    "                                   )\n",
    "\n",
    "\n",
    "    model = Pipeline([('tf_idf', tf_idf),\n",
    "                    ('classifier', classifier)])\n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "    cv_results = cross_validate(model, x_train, y_train,\n",
    "                                scoring=['f1_micro', 'f1_macro', 'f1_weighted'],\n",
    "                                cv=skf)\n",
    "\n",
    "\n",
    "    print(\"F1 Micro:\", cv_results['test_f1_micro'].mean())\n",
    "    print(\"F1 Macro:\", cv_results['test_f1_macro'].mean())\n",
    "    print(\"F1 Weighted:\", cv_results['test_f1_weighted'].mean())\n",
    "    \n",
    "\n",
    "    metrics_train = dict()\n",
    "\n",
    "    for metric in ['test_f1_micro', 'test_f1_macro', 'test_f1_weighted']:\n",
    "        metrics_train[f'{metric[5:]}_mean'] = cv_results[metric].mean()\n",
    "        metrics_train[f'{metric[5:]}_std'] = cv_results[metric].std()\n",
    "        \n",
    "    # Обучение пайплайна на полных данных и прогноз\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    \n",
    "\n",
    "\n",
    "    return model, metrics_train, y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.6771458908913646\n",
      "F1 Macro: 0.6305734687082104\n",
      "F1 Weighted: 0.6769699378739163\n"
     ]
    }
   ],
   "source": [
    "model, metrics_train, y_pred_train, y_pred_test =  tfidf_model_with_cv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_micro_mean': 0.6771458908913646,\n",
       " 'f1_micro_std': 0.010364696149342937,\n",
       " 'f1_macro_mean': 0.6305734687082104,\n",
       " 'f1_macro_std': 0.010530161359530003,\n",
       " 'f1_weighted_mean': 0.6769699378739163,\n",
       " 'f1_weighted_std': 0.010592654773574963}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(df['text_proc4'], df['author_id'], test_size = test_size, random_state=seed,\\\n",
    "#                                                     stratify=df['author_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if y_pred.all() == preds_test.all():\n",
    "#     print(\"Массивы эквивалентны\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_test, y_pred):\n",
    "\n",
    "    metrics_test = dict()\n",
    "\n",
    "    for average in ['micro', 'weighted', 'macro']:\n",
    "        metrics_test[f'f1_{average}'] = f1_score(y_test, y_pred, average=average)\n",
    "\n",
    "    precision_recall_f1_test_df = pd.DataFrame(\n",
    "        precision_recall_fscore_support(y_test, y_pred),\n",
    "        columns=df.author_id.unique().tolist(),\n",
    "        index = ['precision_test', 'recall_test', 'f1_test', 'support']).T.drop(columns=['support']).reset_index()\n",
    "\n",
    "    precision_recall_f1_test_df.rename(columns={'index': 'author'}, inplace=True)\n",
    "    \n",
    "    return metrics_test, precision_recall_f1_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test, precision_recall_f1_test_df = calc_metrics(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.3718473074301295,\n",
       " 'f1_weighted': 0.375705659314937,\n",
       " 'f1_macro': 0.3429279196304781}"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.440994</td>\n",
       "      <td>0.445141</td>\n",
       "      <td>0.443058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.356021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.424286</td>\n",
       "      <td>0.452744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>0.274112</td>\n",
       "      <td>0.263415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.341246</td>\n",
       "      <td>0.347432</td>\n",
       "      <td>0.344311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.226601</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.247978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.275974</td>\n",
       "      <td>0.332031</td>\n",
       "      <td>0.301418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.227027</td>\n",
       "      <td>0.270968</td>\n",
       "      <td>0.247059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0.429603</td>\n",
       "      <td>0.391447</td>\n",
       "      <td>0.409639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author  precision_test  recall_test   f1_test\n",
       "0       6        0.440994     0.445141  0.443058\n",
       "1       0        0.343434     0.369565  0.356021\n",
       "2       7        0.485294     0.424286  0.452744\n",
       "3       3        0.253521     0.274112  0.263415\n",
       "4       1        0.341246     0.347432  0.344311\n",
       "5       9        0.226601     0.273810  0.247978\n",
       "6       5        0.275974     0.332031  0.301418\n",
       "7       2        0.376238     0.351852  0.363636\n",
       "8       4        0.227027     0.270968  0.247059\n",
       "9       8        0.429603     0.391447  0.409639"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f1_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_train, precision_recall_f1_train_df = calc_metrics(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_micro': 0.8773488213187564,\n",
       " 'f1_weighted': 0.8771457884961157,\n",
       " 'f1_macro': 0.8816578109866402}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>precision_test</th>\n",
       "      <th>recall_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.866267</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.875883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.975265</td>\n",
       "      <td>0.961672</td>\n",
       "      <td>0.968421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.877298</td>\n",
       "      <td>0.880359</td>\n",
       "      <td>0.878826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.861301</td>\n",
       "      <td>0.828666</td>\n",
       "      <td>0.844668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.857728</td>\n",
       "      <td>0.869295</td>\n",
       "      <td>0.863472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.868030</td>\n",
       "      <td>0.838420</td>\n",
       "      <td>0.852968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.832468</td>\n",
       "      <td>0.810367</td>\n",
       "      <td>0.821268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.952663</td>\n",
       "      <td>0.952663</td>\n",
       "      <td>0.952663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.875556</td>\n",
       "      <td>0.834746</td>\n",
       "      <td>0.854664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0.892114</td>\n",
       "      <td>0.915683</td>\n",
       "      <td>0.903745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author  precision_test  recall_test   f1_test\n",
       "0       6        0.866267     0.885714  0.875883\n",
       "1       0        0.975265     0.961672  0.968421\n",
       "2       7        0.877298     0.880359  0.878826\n",
       "3       3        0.861301     0.828666  0.844668\n",
       "4       1        0.857728     0.869295  0.863472\n",
       "5       9        0.868030     0.838420  0.852968\n",
       "6       5        0.832468     0.810367  0.821268\n",
       "7       2        0.952663     0.952663  0.952663\n",
       "8       4        0.875556     0.834746  0.854664\n",
       "9       8        0.892114     0.915683  0.903745"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_f1_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## пиклим и загружаем в s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вариант загрузки с локальным файлом\n",
    "# with open('tfidf_logreg_pipeline.pkl', 'wb') as file:\n",
    "#     pickle.dump((model), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вот так загружаем\n",
    "# with open('tfidf_logreg_pipeline.pkl', 'rb') as file:\n",
    "#     tfidf_logreg_pipeline = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification/ML/' #это локальный путь! Куда сохранился мой csv файл\n",
    "# NEW_FILE_NAME = 'tfidf_logreg_pipeline.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3.upload_file(f'{DATA_PATH}{NEW_FILE_NAME}', BUCKET_NAME, f'models/{NEW_FILE_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'dc5ccc9ad99dd59c',\n",
       "  'HostId': '',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'nginx',\n",
       "   'date': 'Tue, 05 Dec 2023 18:05:50 GMT',\n",
       "   'content-type': 'application/octet-stream',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'connection': 'keep-alive',\n",
       "   'keep-alive': 'timeout=60',\n",
       "   'etag': '\"a53c37f532c5bfd235b15b1ede148c52\"',\n",
       "   'x-amz-request-id': 'dc5ccc9ad99dd59c'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"a53c37f532c5bfd235b15b1ede148c52\"'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_buffer = BytesIO()\n",
    "pickle_byte_obj = pickle.dump(model, pickle_buffer)\n",
    "\n",
    "s3.put_object(Body=pickle_buffer.getvalue(), \n",
    "              Bucket=BUCKET_NAME, \n",
    "              Key=f'models/{model_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
