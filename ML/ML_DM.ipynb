{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dariamishina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dariamishina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from tqdm import trange\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "\n",
    "my_stopwords = [] #заглушка если придумаем стопслова\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "russian_stopwords_new = stopwords.words(\"russian\")\n",
    "russian_stopwords_new.extend(my_stopwords)\n",
    "not_stopwords = {'не', 'ни'}\n",
    "russian_stopwords = [word for word in russian_stopwords_new if word not in not_stopwords]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification/ML', '/Users/dariamishina/.conda/envs/untitled/lib/python38.zip', '/Users/dariamishina/.conda/envs/untitled/lib/python3.8', '/Users/dariamishina/.conda/envs/untitled/lib/python3.8/lib-dynload', '', '/Users/dariamishina/.conda/envs/untitled/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# добавляем в path папку с тем файлом, откуда надо импортировать функции\n",
    "sys.path.append('/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dariamishina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dariamishina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_preproc.src.utils import preprocess_text1, preprocess_text2, preprocess_text3, preprocess_text4, tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    endpoint_url='https://storage.yandexcloud.net',\n",
    "    aws_access_key_id='YCAJErlaldUmioGbHQSqJ70MR',\n",
    "    aws_secret_access_key='YCPSba_JgloNYSNWcnKO2CYCEB8PFR1Iwgr2jIUy',\n",
    "    region_name='ru-cental1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем уже расспличенные тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"mlds23-authorship-identification\"\n",
    "BUCKET_DIR = \"splitted_data/\"\n",
    "FILENAME = 'splitted_df.csv'\n",
    "#загружает в локальную директорию, потом отдельно надо считывать\n",
    "s3.download_file(Filename=FILENAME, Bucket=BUCKET_NAME, Key=BUCKET_DIR + FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('splitted_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>Узкими горными тропинками , от одного дачного ...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>свернутый ковер для акробатических упражнений ...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>позабытые . Кроме того , были в шарманке две п...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>тайной грусти : — Что поделаешь ? .. Древний о...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>, да уж ладно ! Кормила она нас с тобой , Серг...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text  \\\n",
       "0  author_id_06  Узкими горными тропинками , от одного дачного ...   \n",
       "1  author_id_06  свернутый ковер для акробатических упражнений ...   \n",
       "2  author_id_06  позабытые . Кроме того , были в шарманке две п...   \n",
       "3  author_id_06  тайной грусти : — Что поделаешь ? .. Древний о...   \n",
       "4  author_id_06  , да уж ладно ! Кормила она нас с тобой , Серг...   \n",
       "\n",
       "                                         book  \n",
       "0  raw_data/aleksandr_kuprin_belyj_pudel'.txt  \n",
       "1  raw_data/aleksandr_kuprin_belyj_pudel'.txt  \n",
       "2  raw_data/aleksandr_kuprin_belyj_pudel'.txt  \n",
       "3  raw_data/aleksandr_kuprin_belyj_pudel'.txt  \n",
       "4  raw_data/aleksandr_kuprin_belyj_pudel'.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Выделяем классы метками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "df['author_id'] = enc.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_id\n",
       "1     379\n",
       "7     446\n",
       "8     627\n",
       "5     725\n",
       "3     804\n",
       "6    1047\n",
       "4    1295\n",
       "0    1299\n",
       "9    2387\n",
       "2    2706\n",
       "Name: author_id, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#посмотрим на распределение классов\n",
    "df.groupby('author_id').author_id.count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# подготовка для TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#используем самую \"суровую предобработку\" с лемматизацией\n",
    "df['text_proc1'] = df['text'].apply(lambda x: preprocess_text1(x, tostr=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>book</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text_proc1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>Узкими горными тропинками , от одного дачного ...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>узкий горный тропинка дачный поселок пробирать...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>свернутый ковер для акробатических упражнений ...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>свернутый ковер акробатический упражнение прав...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>позабытые . Кроме того , были в шарманке две п...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>позабывать кроме шарманка предательский труба ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>тайной грусти : — Что поделаешь ? .. Древний о...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>тайный грусть   —  поделать древний орган прос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author_id_06</td>\n",
       "      <td>, да уж ладно ! Кормила она нас с тобой , Серг...</td>\n",
       "      <td>raw_data/aleksandr_kuprin_belyj_pudel'.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>ладно кормить сергей сей пора бог давать покор...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text  \\\n",
       "0  author_id_06  Узкими горными тропинками , от одного дачного ...   \n",
       "1  author_id_06  свернутый ковер для акробатических упражнений ...   \n",
       "2  author_id_06  позабытые . Кроме того , были в шарманке две п...   \n",
       "3  author_id_06  тайной грусти : — Что поделаешь ? .. Древний о...   \n",
       "4  author_id_06  , да уж ладно ! Кормила она нас с тобой , Серг...   \n",
       "\n",
       "                                         book  author_id  \\\n",
       "0  raw_data/aleksandr_kuprin_belyj_pudel'.txt          6   \n",
       "1  raw_data/aleksandr_kuprin_belyj_pudel'.txt          6   \n",
       "2  raw_data/aleksandr_kuprin_belyj_pudel'.txt          6   \n",
       "3  raw_data/aleksandr_kuprin_belyj_pudel'.txt          6   \n",
       "4  raw_data/aleksandr_kuprin_belyj_pudel'.txt          6   \n",
       "\n",
       "                                          text_proc1  \n",
       "0  узкий горный тропинка дачный поселок пробирать...  \n",
       "1  свернутый ковер акробатический упражнение прав...  \n",
       "2  позабывать кроме шарманка предательский труба ...  \n",
       "3  тайный грусть   —  поделать древний орган прос...  \n",
       "4  ладно кормить сергей сей пора бог давать покор...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['text_proc1'], df['author_id'], test_size = 0.3, random_state=42,\\\n",
    "                                                    stratify=df['author_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdariamishina2812\u001b[0m (\u001b[33mdatalabai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification/ML/wandb/run-20231126_192233-o73ma4wb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlds23_ai/authorship_identification/runs/o73ma4wb' target=\"_blank\">tfidf_log</a></strong> to <a href='https://wandb.ai/mlds23_ai/authorship_identification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlds23_ai/authorship_identification' target=\"_blank\">https://wandb.ai/mlds23_ai/authorship_identification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlds23_ai/authorship_identification/runs/o73ma4wb' target=\"_blank\">https://wandb.ai/mlds23_ai/authorship_identification/runs/o73ma4wb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"authorship_identification\", name=\"tfidf_log\", entity=\"mlds23_ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выглядит не очень, почему-то только один класс, видимо для бинарной классификации только\n",
    "wandb.sklearn.plot_class_proportions(y_train, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model(x_train, y_train):\n",
    "    tf_idf = TfidfVectorizer(stop_words=russian_stopwords, max_features=550, min_df=2)\n",
    "    \n",
    "    logit = LogisticRegression(C=1e2, n_jobs=4, solver='sag',\n",
    "                               random_state=42, verbose=0,\n",
    "                               multi_class='multinomial',\n",
    "                               fit_intercept=True, max_iter=400)\n",
    "   \n",
    "    tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), ('logit', logit)])\n",
    "    \n",
    "    tfidf_logit_pipeline.fit(x_train, y_train)\n",
    "    \n",
    "    # параметры в WandB\n",
    "    wandb.config.update({\n",
    "        'max_features': 550,\n",
    "        'min_df': 2,\n",
    "        'C': 1e2,\n",
    "        'solver': 'sag',\n",
    "        'n_jobs': 4,\n",
    "        'random_state': 42,\n",
    "        'multi_class': 'multinomial',\n",
    "        'fit_intercept': True,\n",
    "        'max_iter': 400\n",
    "    })\n",
    "    \n",
    "    return tfidf_logit_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb.plots.* functions are deprecated and will be removed in a future release. Please use wandb.plot.* instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.211 MB of 0.211 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 1.2%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score_macro</td><td>▁</td></tr><tr><td>f1_score_micro</td><td>▁</td></tr><tr><td>f1_score_weighted</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score_macro</td><td>0.59051</td></tr><tr><td>f1_score_micro</td><td>0.63642</td></tr><tr><td>f1_score_weighted</td><td>0.63647</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tfidf_log</strong> at: <a href='https://wandb.ai/mlds23_ai/authorship_identification/runs/o73ma4wb' target=\"_blank\">https://wandb.ai/mlds23_ai/authorship_identification/runs/o73ma4wb</a><br/> View job at <a href='https://wandb.ai/mlds23_ai/authorship_identification/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExODg5MjQyOQ==/version_details/v1' target=\"_blank\">https://wandb.ai/mlds23_ai/authorship_identification/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExODg5MjQyOQ==/version_details/v1</a><br/>Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231126_192233-o73ma4wb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_logit_pipeline = tfidf_model(x_train, y_train)\n",
    "\n",
    "y_pred = tfidf_logit_pipeline.predict(x_test)\n",
    "y_probas = tfidf_logit_pipeline.predict_proba(x_test)\n",
    "\n",
    "#вот тут модель классификации из пайплайна\n",
    "# tfidf_logit_pipeline['logit']\n",
    "\n",
    "# wandb.sklearn.plot_learning_curve(tfidf_logit_pipeline['logit'], x_train, y_train)\n",
    "#не будет работать для tf-idf,но для статистики можно попробовать\n",
    "\n",
    "# не будет работать для логрега, а для деревьев-бустингов будет\n",
    "# wandb.sklearn.plot_feature_importances(tfidf_logit_pipeline['logit']);\n",
    "\n",
    "# логируем метрики в WandB\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "wandb.log({'f1_score_weighted': f1})\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "wandb.log({'f1_score_micro': f1})\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "wandb.log({'f1_score_macro': f1})\n",
    "\n",
    "#выглядит не очень, почему-то только один класс, видимо для бинарной классификации только\n",
    "wandb.sklearn.plot_roc(y_test, y_probas, labels)\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## просто пайплайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model(x_train, y_train):\n",
    "    \n",
    "    tf_idf = TfidfVectorizer(stop_words=russian_stopwords, max_features=550, min_df=2)\n",
    "    \n",
    "    logit = LogisticRegression(C=1e2, n_jobs=4, solver='sag', #solver='lbfgs', #проблема с этим солвером\n",
    "                           random_state=42, verbose=0, \n",
    "                           multi_class='multinomial',\n",
    "                           fit_intercept=True, max_iter=400)\n",
    "   \n",
    "    tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n",
    "                                 ('logit', logit)])\n",
    "    tfidf_logit_pipeline.fit(x_train, y_train)\n",
    "    return tfidf_logit_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_logit_pipeline = tfidf_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 121 ms, sys: 9.85 ms, total: 130 ms\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = tfidf_logit_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.590514812375762"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6364153627311522"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6364694242915129"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## пиклим и загружаем в s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tfidf_logit_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump((tfidf_logit_pipeline), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вот так загружаем\n",
    "# with open('tfidf_logit_pipeline.pkl', 'rb') as file:\n",
    "#     tfidf_logit_pipeline_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/dariamishina/Documents/HSE/YearProject/mlds23-authorship-identification/ML/' #это локальный путь! Куда сохранился мой csv файл\n",
    "NEW_FILE_NAME = 'tfidf_logit_pipeline.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(f'{DATA_PATH}{NEW_FILE_NAME}', BUCKET_NAME, f'models/{NEW_FILE_NAME}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
